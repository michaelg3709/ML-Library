{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TF2x Pretrained Multiclass Single label Example with TFLite Deployment",
      "provenance": [],
      "collapsed_sections": [
        "bL54LWCHt5q5",
        "T0JEw0InUUZr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5yxnER5P1EF",
        "colab_type": "text"
      },
      "source": [
        "A notebook written in TensorFlow 2.0 designed to use a Pre-trained classifer from TensorFlow Hub to classify pictures of flowers. After running a the model, the model is deployed to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## Set up libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlauq-4FWGZM",
        "outputId": "97c15666-0ed5-4735-b27e-1b90fbc0eb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Version:  2.0.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-MSh-bFTq2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Download TF Hub Module\n",
        "\n",
        "Use one of the selections provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FlsEcKVeuCnf",
        "outputId": "c338f6bf-0f4d-4356-b010-9fbe7d50bbb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "module_selection = (\"inception_v3\", 299) #@param [\"(\\\"mobilenet_v2_035_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\", \"(\\\"inception_resnet_v2\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/classification/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "BATCH_SIZE = 32 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using https://tfhub.dev/google/imagenet/inception_v3/classification/4 with input size (299, 299)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "### Download the Dataset\n",
        "\n",
        "Input URL.\n",
        "Input directory name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBtFK1hO8KsO",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "URL = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz' #@param {type:\"string\"}\n",
        "dir_name = 'flower_photos' #@param {type:\"string\"}\n",
        "\n",
        "data_dir = tf.keras.utils.get_file(dir_name,\n",
        "                                   URL,\n",
        "                                   extract=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxa0-3SQxMRh",
        "colab_type": "text"
      },
      "source": [
        "### *******Understand the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEf8WJauOdWg",
        "colab_type": "text"
      },
      "source": [
        "List the directories with the following terminal command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcBqp7NDN_Zi",
        "colab_type": "code",
        "outputId": "c382a10a-1807-452a-9597-82d97d30e50c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "data_dir_base = os.path.dirname(data_dir)\n",
        "!find $data_dir_base -type d -print"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets\n",
            "/root/.keras/datasets/flower_photos\n",
            "/root/.keras/datasets/flower_photos/roses\n",
            "/root/.keras/datasets/flower_photos/tulips\n",
            "/root/.keras/datasets/flower_photos/dandelion\n",
            "/root/.keras/datasets/flower_photos/daisy\n",
            "/root/.keras/datasets/flower_photos/sunflowers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc3_gOkuw66C",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umB5tswsfTEQ",
        "outputId": "13afb834-c449-4384-fb48-6313529b1d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, \n",
        "                       batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "                          **datagen_kwargs)\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "                          data_dir, \n",
        "                          subset=\"validation\", \n",
        "                          shuffle=False, \n",
        "                          **dataflow_kwargs)\n",
        "\n",
        "do_data_augmentation = True #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, \n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2, \n",
        "      zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "  \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                            data_dir, \n",
        "                            subset=\"training\", \n",
        "                            shuffle=True, \n",
        "                            **dataflow_kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 731 images belonging to 5 classes.\n",
            "Found 2939 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNZeDr2j5W19",
        "colab_type": "text"
      },
      "source": [
        "The resulting object is an iterator that returns image_batch, label_batch pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVBVWJP65XVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_batch, label_batch in train_generator:\n",
        "  print(\"Image batch shape: \", image_batch.shape)\n",
        "  print(\"Label batch shape: \", label_batch.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8QeihxT8Nv",
        "colab_type": "text"
      },
      "source": [
        "## Create and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FS_gVStowW3G"
      },
      "source": [
        "\n",
        "### Define the learning model\n",
        "\n",
        "For speed, start out with a non-trainable `feature_extractor_layer`, enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RaJW3XrPyFiF",
        "colab": {}
      },
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50FYNIb1dmJH",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 30))\n",
        "reset_states = ResetStatesCallback()\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)\n",
        "\n",
        "hist = model.fit_generator(\n",
        "            train_generator,\n",
        "            epochs=100, \n",
        "            callbacks=[early_stopping, lr_schedule, reset_states],\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=valid_generator,\n",
        "            validation_steps=validation_steps).history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-mXX6bkS83o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.semilogx(hist[\"lr\"], hist[\"loss\"])\n",
        "plt.axis([1e-4, 1e-1, 0, 1.5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzYiyypnUPZT",
        "colab_type": "text"
      },
      "source": [
        "### Train the updated Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwikdsSRS1cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()\n",
        "\n",
        "reset_states = ResetStatesCallback()\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_checkpoint.h5\", save_best_only=True)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)\n",
        "\n",
        "hist = model.fit_generator(\n",
        "            train_generator,\n",
        "            epochs=100, \n",
        "            callbacks=[early_stopping, model_checkpoint, reset_states],\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=valid_generator,\n",
        "            validation_steps=validation_steps).history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAs0GEUkfAqn",
        "colab_type": "text"
      },
      "source": [
        "MobileNet - acc: 0.856 with aug + dropout no FT\n",
        "\n",
        "Inception - acc: 0.911 with aug, with/without dropout, no FT, lr=le-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFsjbktwcT1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(\"my_checkpoint.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0JEw0InUUZr",
        "colab_type": "text"
      },
      "source": [
        "## Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CYOw0fTO1W4x",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieve a list of accuracy results on training and test datasets for each training epoch\n",
        "acc = hist['acc']\n",
        "val_acc = hist['val_acc']\n",
        "\n",
        "# Retrieve a list of list results on training and test datasets for each training epoch\n",
        "loss = hist['loss']\n",
        "val_loss = hist['val_loss']\n",
        "\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBmyF-Kd45Dt",
        "colab_type": "text"
      },
      "source": [
        "## Check the predictions\n",
        "Get the ordered list of class names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GECMVY6_47y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "class_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d44UclPq4_NG",
        "colab_type": "text"
      },
      "source": [
        "Run the image batch through the model and convert the indices to class names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSrg9D345Aga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_batch = model.predict(image_batch)\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "predicted_label_batch = class_names[predicted_id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RimMEwzk5Chp",
        "colab_type": "text"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT9rGUKM5DBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_id = np.argmax(label_batch, axis=-1)\n",
        "\n",
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(image_batch[n])\n",
        "  color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\n",
        "  plt.title(predicted_label_batch[n].title(), color=color)\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "Generate a saved model for deployment to TF Serving or TF Lite (on mobile) as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGvTi69oIc2d",
        "colab": {}
      },
      "source": [
        "saved_model_path = \"/saved_model/1\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## Deployment to TensorFlow Lite\n",
        "\n",
        "Optimise the model and convert the trained model to TF Lite and apply post-training tools from the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). \n",
        "\n",
        "  * Converting without optimization provides the same results as before (up to roundoff error).\n",
        "  * Converting with optimization without any data quantizes the model weights to 8 bits, but inference still uses floating-point computation for the neural network activations. This reduces model size almost by a factor of 4 and improves CPU latency on mobile devices.\n",
        "  * On top, computation of the neural network activations can be quantized to 8-bit integers as well if a small reference dataset is provided to calibrate the quantization range. On a mobile device, this accelerates inference further and makes it possible to run on accelerators like EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Va1Vo92fSyV6",
        "colab": {}
      },
      "source": [
        "#@title Optimization settings\n",
        "optimize_lite_model = True  #@param {type:\"boolean\"}\n",
        "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
        "num_calibration_examples = 154  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "#@markdown Select mode of optimization\n",
        "mode = \"Speed\" #@param [\"Default\", \"Storage\", \"Speed\"]\n",
        "\n",
        "if mode == 'Storage':\n",
        "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "  optimization = tf.lite.Optimize.DEFAULT\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNmY1eRyDwpp",
        "colab_type": "text"
      },
      "source": [
        "Convert the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA6QDRPgDvZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [optimization]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_file = 'model.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(tflite_model)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7JQSNI6Axv",
        "colab_type": "text"
      },
      "source": [
        "Test the model with the TF Lite Interpreter to examine the resulting quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_wqEmD0xIqeG",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JMMK-fZrKrk8",
        "colab": {}
      },
      "source": [
        "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
        "num_eval_examples = 57  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_generator\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej2tRdS_FduE",
        "colab_type": "text"
      },
      "source": [
        "Download the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maFIYYvCFfFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "\n",
        "  files.download(tflite_model_file)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}